{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a97e8b0f",
   "metadata": {},
   "source": [
    "# Testing multiclass classification and using TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4091f30b",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8503030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x75e60dc32f10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "from contextlib import contextmanager\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6167f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33ed318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data-unversioned/p1ch7/\"\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616)),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac2fec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616)),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b1a0de",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m label_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[1;32m      2\u001b[0m class_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mairplane\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbird\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m cifar2 \u001b[38;5;241m=\u001b[39m [(img, label_map[label]) \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m cifar10 \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m]]\n\u001b[1;32m      4\u001b[0m cifar2_val \u001b[38;5;241m=\u001b[39m [(img, label_map[label]) \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m cifar10_val \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m label_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[1;32m      2\u001b[0m class_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mairplane\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbird\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m cifar2 \u001b[38;5;241m=\u001b[39m [(img, label_map[label]) \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m cifar10 \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m]]\n\u001b[1;32m      4\u001b[0m cifar2_val \u001b[38;5;241m=\u001b[39m [(img, label_map[label]) \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m cifar10_val \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m]]\n",
      "File \u001b[0;32m~/dl/.venv/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/dl/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/dl/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dl/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl/.venv/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:920\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    917\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    919\u001b[0m dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 920\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(std, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (std \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = [\"airplane\", \"bird\"]\n",
    "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374962dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea2ebbd",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620edea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, loss_fn, device):\n",
    "    \"\"\"Run one epoch of training.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader: Training data loader\n",
    "        optimizer: PyTorch optimizer\n",
    "        loss_fn: Loss function\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing training metrics\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_train = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        \n",
    "        outputs = model(imgs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_train += labels.shape[0]\n",
    "        correct_train += int((predicted == labels).sum())\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = loss_train / len(train_loader)\n",
    "    train_accuracy = correct_train / total_train\n",
    "    \n",
    "    return {\n",
    "        'loss': train_loss,\n",
    "        'accuracy': train_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def create_summary_writer(model_name, hyperparams=None):\n",
    "    \"\"\"Context manager for creating and managing a TensorBoard SummaryWriter.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model for the log directory\n",
    "        hyperparams (dict, optional): Hyperparameters to log\n",
    "        \n",
    "    Yields:\n",
    "        SummaryWriter: TensorBoard writer\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    model_architecture = hyperparams.get('model_architecture', '') if hyperparams else ''\n",
    "    log_dir = f\"runs/{model_name}_{model_architecture}_{timestamp}\"\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    print(f\"TensorBoard logs will be saved to {log_dir}\")\n",
    "    \n",
    "    if hyperparams:\n",
    "        # Log hyperparameters as text\n",
    "        param_str = \"\\n\".join([f\"{k}: {v}\" for k, v in hyperparams.items()])\n",
    "        writer.add_text('Hyperparameters', param_str)\n",
    "    \n",
    "    try:\n",
    "        yield writer\n",
    "    finally:\n",
    "        writer.close()\n",
    "        print(f\"TensorBoard writer closed for {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d755fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, loss_fn, device, class_names=None):\n",
    "    \"\"\"Evaluate the model on validation data.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        val_loader: Validation data loader\n",
    "        loss_fn: Loss function\n",
    "        device: Device to run on\n",
    "        class_names (list, optional): List of class names\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing validation metrics and predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total_val += labels.shape[0]\n",
    "            correct_val += int((predicted == labels).sum())\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = correct_val / total_val\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'loss': val_loss,\n",
    "        'accuracy': val_accuracy,\n",
    "        'f1': val_f1,\n",
    "        'predictions': all_preds,\n",
    "        'true_labels': all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d75707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(writer, train_metrics, val_metrics, optimizer, epoch):\n",
    "    \"\"\"Log metrics to TensorBoard.\n",
    "    \n",
    "    Args:\n",
    "        writer: TensorBoard SummaryWriter\n",
    "        train_metrics (dict): Training metrics\n",
    "        val_metrics (dict): Validation metrics\n",
    "        optimizer: PyTorch optimizer\n",
    "        epoch (int): Current epoch\n",
    "    \"\"\"\n",
    "    # Log scalar metrics\n",
    "    writer.add_scalar('Loss/train', train_metrics['loss'], epoch)\n",
    "    writer.add_scalar('Loss/validation', val_metrics['loss'], epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_metrics['accuracy'], epoch)\n",
    "    writer.add_scalar('Accuracy/validation', val_metrics['accuracy'], epoch)\n",
    "    writer.add_scalar('F1/validation', val_metrics['f1'], epoch)\n",
    "    \n",
    "    # Log learning rate\n",
    "    writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0009108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_info(writer, model, epoch, train_loader, device):\n",
    "    \"\"\"Log model parameters and gradients to TensorBoard.\n",
    "    \n",
    "    Args:\n",
    "        writer: TensorBoard SummaryWriter\n",
    "        model: PyTorch model\n",
    "        epoch (int): Current epoch\n",
    "        train_loader: Training data loader\n",
    "        device: Device to run on\n",
    "    \"\"\"\n",
    "    # Log model graph (only once)\n",
    "    if epoch == 1:\n",
    "        example_images, _ = next(iter(train_loader))\n",
    "        try:\n",
    "            writer.add_graph(model, example_images.to(device))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to add model graph to TensorBoard: {e}\")\n",
    "    \n",
    "    # Log histograms of model parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram(f'Parameters/{name}', param, epoch)\n",
    "        if param.grad is not None:\n",
    "            writer.add_histogram(f'Gradients/{name}', param.grad, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_predictions(writer, model, val_loader, device, class_names, epoch, num_images=10):\n",
    "    \"\"\"Log prediction visualizations to TensorBoard.\n",
    "    \n",
    "    Args:\n",
    "        writer: TensorBoard SummaryWriter\n",
    "        model: PyTorch model\n",
    "        val_loader: Validation data loader\n",
    "        device: Device to run on\n",
    "        class_names (list): List of class names\n",
    "        epoch (int): Current epoch\n",
    "        num_images (int): Number of images to visualize\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            for j in range(imgs.size()[0]):\n",
    "                if images_so_far >= num_images:\n",
    "                    break\n",
    "                \n",
    "                ax = plt.subplot(2, num_images//2, images_so_far + 1)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'pred: {class_names[preds[j]]}\\ntrue: {class_names[labels[j]]}',\n",
    "                           color=(\"green\" if preds[j]==labels[j] else \"red\"))\n",
    "                \n",
    "                # Denormalize and convert to numpy for matplotlib\n",
    "                img = imgs[j].cpu().numpy().transpose((1, 2, 0))\n",
    "                mean = np.array([0.4915, 0.4823, 0.4468])\n",
    "                std = np.array([0.2470, 0.2435, 0.2616])\n",
    "                img = std * img + mean\n",
    "                img = np.clip(img, 0, 1)\n",
    "                \n",
    "                plt.imshow(img)\n",
    "                images_so_far += 1\n",
    "                if images_so_far >= num_images:\n",
    "                    break\n",
    "    \n",
    "    writer.add_figure(f'Predictions/Epoch_{epoch}', fig, epoch)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20991202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_embeddings(writer, model, val_loader, device, class_names, n_epochs):\n",
    "    \"\"\"Log embeddings to TensorBoard.\n",
    "    \n",
    "    Args:\n",
    "        writer: TensorBoard SummaryWriter\n",
    "        model: PyTorch model\n",
    "        val_loader: Validation data loader\n",
    "        device: Device to run on\n",
    "        class_names (list): List of class names\n",
    "        n_epochs (int): Total number of epochs\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Get features from the last layer before classification\n",
    "    def hook_fn(module, input, output):\n",
    "        features.append(input[0].cpu().numpy())\n",
    "    \n",
    "    # Register hook to the second-to-last layer\n",
    "    try:\n",
    "        handle = model.fc1.register_forward_hook(hook_fn)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                model(imgs)\n",
    "                labels_list.extend(labels.numpy())\n",
    "        \n",
    "        handle.remove()\n",
    "        \n",
    "        # Concatenate all features\n",
    "        features = np.concatenate(features)\n",
    "        \n",
    "        # Select a subset of data for visualization (max 10000 points)\n",
    "        max_samples = min(10000, len(features))\n",
    "        indices = np.random.choice(len(features), max_samples, replace=False)\n",
    "        \n",
    "        # Log embeddings\n",
    "        writer.add_embedding(\n",
    "            features[indices],\n",
    "            metadata=[class_names[l] for l in np.array(labels_list)[indices]],\n",
    "            label_img=None,\n",
    "            global_step=n_epochs\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to log embeddings to TensorBoard: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08739c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to terminate training when validation loss doesn't improve.\n",
    "    \n",
    "    Args:\n",
    "        patience (int): How many epochs to wait after last improvement.\n",
    "        min_delta (float): Minimum change to qualify as an improvement.\n",
    "        mode (str): 'min' for monitoring metrics that decrease (like loss),\n",
    "                    'max' for metrics that increase (like accuracy).\n",
    "        verbose (bool): If True, prints a message for each improvement.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0.0, mode='min', verbose=False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "        # Set the direction based on mode\n",
    "        self.monitor_op = np.less if mode == 'min' else np.greater\n",
    "        self.min_delta = min_delta if mode == 'min' else -min_delta\n",
    "        \n",
    "    def __call__(self, epoch, current_score, model=None, path=None):\n",
    "        \"\"\"Check if training should be stopped.\n",
    "        \n",
    "        Args:\n",
    "            epoch (int): Current epoch number\n",
    "            current_score (float): Current validation metric to monitor\n",
    "            model (torch.nn.Module, optional): Model to save if score improves\n",
    "            path (str, optional): Path to save the model\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if training should stop, False otherwise\n",
    "        \"\"\"\n",
    "        if self.best_score is None:\n",
    "            # First epoch\n",
    "            self.best_score = current_score\n",
    "            self.best_epoch = epoch\n",
    "            self.save_checkpoint(current_score, model, path)\n",
    "        elif self.monitor_op(current_score - self.min_delta, self.best_score):\n",
    "            # Score improved\n",
    "            self.best_score = current_score\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(current_score, model, path)\n",
    "        else:\n",
    "            # Score did not improve\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "        return self.early_stop\n",
    "    \n",
    "    def save_checkpoint(self, score, model, path):\n",
    "        \"\"\"Save model when validation score improves.\"\"\"\n",
    "        if self.verbose:\n",
    "            improved = 'improved' if self.best_score == score else 'did not improve'\n",
    "            metric_name = 'loss' if self.mode == 'min' else 'score'\n",
    "            print(f'Validation {metric_name} {improved} ({self.best_score:.6f} --> {score:.6f})')\n",
    "        \n",
    "        if model is not None and path is not None:\n",
    "            torch.save(model.state_dict(), path)\n",
    "            if self.verbose:\n",
    "                print(f'Model saved to {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader, val_loader, \n",
    "                  epoch_trainer, device, class_names, model_name=\"model\", \n",
    "                  early_stopping_params=None):\n",
    "    \"\"\"Main training loop with TensorBoard logging and early stopping.\n",
    "    \n",
    "    Args:\n",
    "        n_epochs (int): Maximum number of epochs\n",
    "        optimizer: PyTorch optimizer\n",
    "        model: PyTorch model\n",
    "        loss_fn: Loss function\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        device: Device to run on\n",
    "        class_names (list): List of class names\n",
    "        model_name (str): Name for the model in logs\n",
    "        early_stopping_params (dict, optional): Parameters for early stopping\n",
    "            {\n",
    "                'patience': int,\n",
    "                'min_delta': float,\n",
    "                'metric': str ('loss', 'accuracy', or 'f1'),\n",
    "                'mode': str ('min' or 'max')\n",
    "            }\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing best metrics\n",
    "    \"\"\"\n",
    "    print(f\"Training on device {device}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device=device)\n",
    "    \n",
    "    # Setup early stopping if parameters are provided\n",
    "    early_stopping = None\n",
    "    if early_stopping_params:\n",
    "        metric = early_stopping_params.get('metric', 'f1')\n",
    "        mode = early_stopping_params.get('mode', 'min' if metric == 'loss' else 'max')\n",
    "        patience = early_stopping_params.get('patience', 10)\n",
    "        min_delta = early_stopping_params.get('min_delta', 0.0)\n",
    "        verbose = early_stopping_params.get('verbose', False)\n",
    "        \n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=patience,\n",
    "            min_delta=min_delta,\n",
    "            mode=mode,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        print(f\"Early stopping enabled: monitoring {metric}, mode={mode}, patience={patience}\")\n",
    "    \n",
    "    # Create hyperparameters dict for logging\n",
    "    hyperparams = {\n",
    "        'batch_size': train_loader.batch_size,\n",
    "        'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "        'weight_decay': optimizer.param_groups[0].get('weight_decay', 0),\n",
    "        'epochs': n_epochs,\n",
    "        'optimizer': optimizer.__class__.__name__,\n",
    "        'model_architecture': model.__class__.__name__,\n",
    "    }\n",
    "    \n",
    "    if early_stopping_params:\n",
    "        hyperparams.update({\n",
    "            'early_stopping_metric': early_stopping_params.get('metric', 'loss'),\n",
    "            'early_stopping_patience': early_stopping_params.get('patience', 10),\n",
    "            'early_stopping_min_delta': early_stopping_params.get('min_delta', 0.0),\n",
    "        })\n",
    "    \n",
    "    # Create TensorBoard writer using context manager\n",
    "    with create_summary_writer(model_name, hyperparams) as writer:\n",
    "        best_val_f1 = 0.0\n",
    "        best_metrics = {}\n",
    "        \n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            # Train for one epoch\n",
    "            train_metrics = epoch_trainer(model, train_loader, optimizer, loss_fn, device)\n",
    "            \n",
    "            # Evaluate the model\n",
    "            val_metrics = evaluate_model(model, val_loader, loss_fn, device, class_names)\n",
    "            \n",
    "            # Log metrics to TensorBoard\n",
    "            log_metrics(writer, train_metrics, val_metrics, optimizer, epoch)\n",
    "            \n",
    "            # Log model information periodically\n",
    "            if epoch % 10 == 0 or epoch == 1:\n",
    "                log_model_info(writer, model, epoch, train_loader, device)\n",
    "                log_predictions(writer, model, val_loader, device, class_names, epoch)\n",
    "            \n",
    "            # Print metrics periodically\n",
    "            if epoch == 1 or epoch % 5 == 0:\n",
    "                print(f\"{datetime.datetime.now()} Epoch {epoch}\")\n",
    "                print(f\"  Training:   Loss: {train_metrics['loss']:.4f}, Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "                print(f\"  Validation: Loss: {val_metrics['loss']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "            \n",
    "            # Save best model based on F1 score\n",
    "            if val_metrics['f1'] > best_val_f1:\n",
    "                best_val_f1 = val_metrics['f1']\n",
    "                best_metrics = val_metrics.copy()\n",
    "                torch.save(model.state_dict(), f\"runs/{model_name}_best_model.pth\")\n",
    "            \n",
    "            # Check early stopping condition\n",
    "            if early_stopping:\n",
    "                # Get the metric to monitor\n",
    "                metric_name = early_stopping_params.get('metric', 'loss')\n",
    "                current_metric = val_metrics[metric_name]\n",
    "                \n",
    "                # Call early stopping with current metric\n",
    "                model_path = f\"runs/{model_name}_early_stopping.pth\"\n",
    "                if early_stopping(epoch, current_metric, model, model_path):\n",
    "                    print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                    print(f\"Best {metric_name} was at epoch {early_stopping.best_epoch}\")\n",
    "                    break\n",
    "        \n",
    "        # Log embeddings after training is complete\n",
    "        log_embeddings(writer, model, val_loader, device, class_names, epoch)\n",
    "        \n",
    "        # Log final hyperparameters with metrics\n",
    "        final_metrics = {\n",
    "            'hparam/val_accuracy': val_metrics['accuracy'],\n",
    "            'hparam/val_f1': val_metrics['f1'],\n",
    "            'hparam/val_loss': val_metrics['loss'],\n",
    "            'hparam/epochs_trained': epoch\n",
    "        }\n",
    "        writer.add_hparams(hyperparams, final_metrics)\n",
    "    \n",
    "    # Set the model to evaluation mode after training\n",
    "    model.eval()\n",
    "    print(f\"Training complete. Best validation F1: {best_val_f1:.4f}\")\n",
    "    \n",
    "    # If early stopping was used, report the best epoch\n",
    "    if early_stopping:\n",
    "        print(f\"Best {early_stopping_params.get('metric', 'loss')} was at epoch {early_stopping.best_epoch}\")\n",
    "    \n",
    "    return best_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7312eb",
   "metadata": {},
   "source": [
    "## Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd4b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=512, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=512, shuffle=False)\n",
    "all_acc_dict = collections.OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463554aa",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8763e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cifar2_model\"\n",
    "n_epochs = 200\n",
    "early_stopping_params = {\n",
    "    'metric': 'f1',      # Monitor F1 score\n",
    "    'mode': 'max',       # We want to maximize F1\n",
    "    'patience': 5,       # Wait for 5 epochs before stopping\n",
    "    'min_delta': 0.001   # Minimum change to qualify as improvement\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14c6c8",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f394b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transfer the model (all its parameters) to the device.\n",
    "model = Net().to(device=device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "all_acc_dict[\"baseline\"] = training_loop(\n",
    "    n_epochs=n_epochs,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    epoch_trainer=train_epoch,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    class_names=class_names,\n",
    "    model_name=model_name,\n",
    "    early_stopping_params=early_stopping_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d023541",
   "metadata": {},
   "source": [
    "### Augmenting width or channels in convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetWidth(n_chans1=32).to(device=device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "all_acc_dict[\"width\"] = training_loop(\n",
    "    n_epochs=n_epochs,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    epoch_trainer=train_epoch,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    class_names=class_names,\n",
    "    model_name=model_name,\n",
    "    early_stopping_params=early_stopping_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d43c0",
   "metadata": {},
   "source": [
    "### $L_2$-regularizarion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e3e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_epoch_trainer(model, train_loader, optimizer, loss_fn, device):\n",
    "    \"\"\"Run one epoch of training.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader: Training data loader\n",
    "        optimizer: PyTorch optimizer\n",
    "        loss_fn: Loss function\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing training metrics\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_train = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        \n",
    "        outputs = model(imgs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        l2_lambda = 0.001\n",
    "        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "        loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_train += labels.shape[0]\n",
    "        correct_train += int((predicted == labels).sum())\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = loss_train / len(train_loader)\n",
    "    train_accuracy = correct_train / total_train\n",
    "    \n",
    "    return {\n",
    "        'loss': train_loss,\n",
    "        'accuracy': train_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4392a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device=device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "all_acc_dict[\"l2 reg\"] = training_loop(\n",
    "    n_epochs=n_epochs,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    epoch_trainer=l2_epoch_trainer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    class_names=class_names,\n",
    "    model_name=model_name,\n",
    "    early_stopping_params=early_stopping_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26c602",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e161cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetDropout(n_chans1=32).to(device=device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "all_acc_dict[\"dropout\"] = training_loop(\n",
    "    n_epochs=n_epochs,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    epoch_trainer=train_epoch,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    class_names=class_names,\n",
    "    model_name=model_name,\n",
    "    early_stopping_params=early_stopping_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9a003",
   "metadata": {},
   "source": [
    "### Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e234fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d9237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetBatchNorm(n_chans1=32).to(device=device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "all_acc_dict[\"batch_norm\"] = training_loop(\n",
    "    n_epochs=n_epochs,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    epoch_trainer=train_epoch,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    class_names=class_names,\n",
    "    model_name=model_name,\n",
    "    early_stopping_params=early_stopping_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce573c8",
   "metadata": {},
   "source": [
    "### Augmenting depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd2629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25858d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetDepth(n_chans1=32).to(device=device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "all_acc_dict[\"depth\"] = training_loop(\n",
    "    n_epochs=n_epochs,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    epoch_trainer=train_epoch,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    class_names=class_names,\n",
    "    model_name=model_name,\n",
    "    early_stopping_params=early_stopping_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2847bbab",
   "metadata": {},
   "source": [
    "### Residual connections (ResNets) or skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e4acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetRes(n_chans1=32).to(device=device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "all_acc_dict[\"res\"] = training_loop(\n",
    "    n_epochs=n_epochs,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    epoch_trainer=train_epoch,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    class_names=class_names,\n",
    "    model_name=model_name,\n",
    "    early_stopping_params=early_stopping_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            n_chans, n_chans, kernel_size=3, padding=1, bias=False\n",
    "        )  # <1>\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity=\"relu\")  # <2>\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(*(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede00ff0",
   "metadata": {},
   "source": [
    "## Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_evaluation(model_class, model_path, test_loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Perform final evaluation of the model on the test set.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model weights\n",
    "        test_loader: DataLoader for test data\n",
    "        device: Device to run on\n",
    "        class_names: List of class names\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing test metrics\n",
    "    \"\"\"\n",
    "    # Initialize model and load weights\n",
    "    model = model_class().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    test_metrics = evaluate_model(model, test_loader, loss_fn, device, class_names)\n",
    "    \n",
    "    # Print comprehensive results\n",
    "    print(\"\\n===== FINAL MODEL EVALUATION =====\")\n",
    "    print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    y_true = np.array(test_metrics['true_labels'])\n",
    "    y_pred = np.array(test_metrics['predictions'])\n",
    "    \n",
    "    # Precision, recall per class\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "    \n",
    "    print(\"\\nPer-class performance:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"{class_name:10s}: Precision: {precision[i]:.4f}, Recall: {recall[i]:.4f}, F1: {f1[i]:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    # Add text annotations to confusion matrix\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "    return test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d29a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_path = \"runs/cifar10_cnn_best_model.pth\"\n",
    "# test_metrics = final_evaluation(best_model_path, val_loader, device, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
